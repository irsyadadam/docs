---
title: "Quickstart"
description: "Get up and running with Standard Model in minutes"
icon: "rocket"
---

Get started with Standard Model Bio's biological world model. This guide walks you through setting up your environment and downloading the Standard Model v1.

<Info>

**New:** This quickstart now features [Standard Model v1](https://huggingface.co/standardmodelbio/SMB-v1-1.7B-Structure) — our flagship JEPA-based multimodal foundation model for oncology.

</Info>

## Prerequisites

Before you begin, ensure you have the following installed:

- **Python 3.10+** — Required for running the models
- **pip** — Python package manager
- **CUDA** (recommended) — For GPU acceleration with NVIDIA GPUs
- **Git** — For cloning repositories

<Tip>

GPU support is strongly recommended. The Standard Model v1 (1.7B parameters) requires approximately 16GB GPU memory for inference.

</Tip>

## One-Command Setup (Recommended)

Run the quickstart script to automatically configure your environment:

```bash
curl -fsSL https://raw.githubusercontent.com/standardmodelbio/quickstart/main/quickstart.sh | bash
```

This script will:

1. Create a Python 3.10 virtual environment named `standard_model`
2. Install PyTorch with CUDA support (if available)
3. Install HuggingFace libraries (transformers, datasets, accelerate)
4. Download the Standard Model to your local machine

## Manual Installation

If you prefer to set up your environment manually:

<Steps>

<Step title="Create Virtual Environment">

```bash
python3 -m venv standard_model
source standard_model/bin/activate
```

</Step>

<Step title="Install PyTorch">

<Tabs>

<Tab title="CUDA 12.x">

```bash
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121
```

</Tab>

<Tab title="CUDA 11.x">

```bash
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
```

</Tab>

<Tab title="CPU Only">

```bash
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu
```

</Tab>

</Tabs>

</Step>

<Step title="Install HuggingFace Libraries">

```bash
pip install transformers datasets accelerate huggingface_hub
```

</Step>

<Step title="Download Standard Model v1">

```python
from huggingface_hub import snapshot_download

snapshot_download(
    "standardmodelbio/SMB-v1-1.7B-Structure",
    local_dir="./SMB-v1-1.7B-Structure"
)
```

</Step>

</Steps>

## Verify Installation

Verify that everything is working correctly:

```python
import torch
from transformers import AutoModel, AutoTokenizer

# Check PyTorch and CUDA
print(f"PyTorch version: {torch.__version__}")
print(f"CUDA available: {torch.cuda.is_available()}")

# Load Standard Model v1
model_path = "./SMB-v1-1.7B-Structure"
model = AutoModel.from_pretrained(model_path)
tokenizer = AutoTokenizer.from_pretrained(model_path)

print("Standard Model v1 loaded successfully!")
print(f"Model parameters: {sum(p.numel() for p in model.parameters()):,}")
```

## Basic Usage

The Standard Model v1 is a biological world model that predicts patient state trajectories:

```python
from transformers import AutoModel, AutoTokenizer
import torch

# Load model
model = AutoModel.from_pretrained("standardmodelbio/SMB-v1-1.7B-Structure")
tokenizer = AutoTokenizer.from_pretrained("standardmodelbio/SMB-v1-1.7B-Structure")

# Move to GPU if available
device = "cuda" if torch.cuda.is_available() else "cpu"
model = model.to(device)

# The Standard Model predicts future patient states
# given current state S(t) and intervention A(t)
# See full documentation for trajectory prediction API
```

<Warning>

The Standard Model v1 is a **world model**, not a text generator. It predicts patient states in latent space, not tokens. See the [Models documentation](/models) for the full API.

</Warning>

## Environment Activation

After setup, activate your environment for future sessions:

```bash
source standard_model/bin/activate
```

## Download Other Models

Download additional models from the Standard Model family:

<CodeGroup>

```python SMB-EHR-4B
from huggingface_hub import snapshot_download

snapshot_download(
    "standardmodelbio/smb-ehr-4b",
    local_dir="./smb-ehr-4b"
)
```

```python SMB-Vision-Base
from huggingface_hub import snapshot_download

snapshot_download(
    "standardmodelbio/smb-vision-base",
    local_dir="./smb-vision-base"
)
```

```python SMB-Language-8B
from huggingface_hub import snapshot_download

snapshot_download(
    "standardmodelbio/model-model-smb-mntp-llama-3.1-8b-v1",
    local_dir="./smb-language-8b"
)
```

</CodeGroup>

## Troubleshooting

<CardGroup cols={2}>

<Card title="CUDA Not Detected" icon="microchip">

Ensure NVIDIA drivers are up to date. Run `nvidia-smi` to verify GPU is accessible.

</Card>

<Card title="Out of Memory" icon="memory">

Standard Model v1 requires ~16GB GPU memory. Use `torch.float16` or quantization for smaller GPUs.

</Card>

<Card title="Model Access Denied" icon="lock">

Some models may require authentication. Run `huggingface-cli login` with your token.

</Card>

<Card title="Slow Download" icon="gauge">

Model downloads can be large (several GB). Ensure stable connection and sufficient disk space.

</Card>

</CardGroup>

### Reducing Memory Usage

For GPUs with less memory, use half-precision or quantization:

```python
import torch
from transformers import AutoModel

# Load in float16 (half precision)
model = AutoModel.from_pretrained(
    "standardmodelbio/SMB-v1-1.7B-Structure",
    torch_dtype=torch.float16,
    device_map="auto"
)

# Or use 8-bit quantization (requires bitsandbytes)
# pip install bitsandbytes
model = AutoModel.from_pretrained(
    "standardmodelbio/SMB-v1-1.7B-Structure",
    load_in_8bit=True,
    device_map="auto"
)
```

## Hardware Requirements

| Model | Parameters | Min GPU Memory | Recommended |
|-------|------------|----------------|-------------|
| Standard Model v1 | 1.7B | 16 GB | 32 GB |
| SMB-EHR-4B | 4B | 24 GB | 48 GB |
| SMB-Vision-Base | 97.2M | 4 GB | 8 GB |
| SMB-Language-8B | 8B | 32 GB | 80 GB |

## Next Steps

<CardGroup cols={2}>

<Card title="Models" icon="cube" href="/models">

Explore the full model catalog and capabilities.

</Card>

<Card title="Architecture" icon="diagram-project" href="/models#featured-standard-model-v1">

Learn how the Standard Model JEPA architecture works.

</Card>

<Card title="Research" icon="book" href="https://blog.standardmodel.bio/p/the-patient-is-not-a-document-moving">

Read the announcement blog post.

</Card>

<Card title="HuggingFace" icon="face-smile" href="https://huggingface.co/standardmodelbio">

Browse all models on HuggingFace.

</Card>

</CardGroup>